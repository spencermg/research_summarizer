import research_summarizer.utils as utils
import research_summarizer.model as model
from pathlib import Path
import pickle
from datetime import datetime, time
import pytz

PARENT_DIR = Path(__file__).resolve().parent.parent
PMCID_PATH = PARENT_DIR / "pmc_ids.pkl"
EST_TIME = datetime.now(pytz.timezone("US/Eastern"))
IS_PEAK_HOURS = EST_TIME.weekday() < 5 and time(5, 0) <= EST_TIME.time() <= time(21, 0)

### TODO: Add ability for users to input their own set of PMC IDs to replace querying
### TODO: Add impact weights for articles using PubTator scores and/or number of citations
### TODO: Figure out why broad queries give "Error: 429"
### TODO: Use tags when applicable

def main():
    out_dir, query, max_articles, num_days, openai_key, anthropic_key, gemini_key = utils._parse_args(PARENT_DIR)

    summarizer = model.llm_summarizer(openai_key, anthropic_key, gemini_key)
    max_articles = utils._handle_num_requests(IS_PEAK_HOURS, max_articles)
    results_dir= Path(out_dir.joinpath(f"results_{datetime.now().strftime('%Y_%m_%d_%H%M%S')}"))
    results_dir.mkdir(parents=True, exist_ok=True)

    if not Path.exists(PMCID_PATH):
        utils.process_pmcid_file(PMCID_PATH)
    
    with open(PMCID_PATH, "rb") as f:
        valid_pmc_ids = pickle.load(f)

    # Fetch articles according to defined search query.
    dict_articles = utils.fetch_articles(query, max_articles, num_days, valid_pmc_ids)
    print(f"{len(dict_articles)} full-text articles found")

    df_articles = utils._combine_article_sections(dict_articles)
    device = utils.find_device()
    df_summaries, df_abstracts = utils.summarize_articles(df_articles, device, summarizer)

    print(df_summaries.head())
    print(df_abstracts.head())    
    


    '''
    # Evaluate the summaries generated by the different models
    bart_scores = rouge.compute(predictions=[summary], references=[abstract])
    falconsai_scores = rouge.compute(predictions=[summary], references=[abstract])
    bigbird_scores = rouge.compute(predictions=[summary], references=[abstract])
    openai_scores = rouge.compute(predictions=[openai_response], references=[abstract])
    anthropic_scores = rouge.compute(predictions=[anthropic_response], references=[abstract])
    genai_scores = rouge.compute(predictions=[gemini_response], references=[abstract])

    # Create a chart to compare the scores
    models = ["BART", "Falconsai", "BigBird", "OpenAI GPT-4o-mini", "Anthropic Claude-3-haiku", "Google Generative AI Gemini-1.5-flash"]
    rouge_1_scores = [bart_scores["rouge1"], falconsai_scores["rouge1"], bigbird_scores["rouge1"], openai_scores["rouge1"], anthropic_scores["rouge1"], genai_scores["rouge1"]]
    rouge_2_scores = [bart_scores["rouge2"], falconsai_scores["rouge2"], bigbird_scores["rouge2"], openai_scores["rouge2"], anthropic_scores["rouge2"], genai_scores["rouge2"]]
    rouge_l_scores = [bart_scores["rougeL"], falconsai_scores["rougeL"], bigbird_scores["rougeL"], openai_scores["rougel"], anthropic_scores["rougel"], genai_scores["rougel"]]
    scores = [rouge_1_scores, rouge_2_scores, rouge_l_scores]
    fig, ax = plt.subplots()
    bar_width = 0.25
    index = range(len(models))
    for i, score in enumerate(scores):
        ax.bar([x + bar_width * i for x in index], score, bar_width, label=f"Score {i + 1}")
    ax.set_xlabel("Model")
    ax.set_ylabel("Score")
    ax.set_title("ROUGE Scores by Model")
    ax.set_xticks([x + bar_width for x in index])
    ax.set_xticklabels(models)
    ax.legend()
    plt.show()
    '''
